{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import gc\n",
    "import json\n",
    "import h5py\n",
    "import threading\n",
    "import time\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import Sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import _remove_long_seq\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, TimeDistributed, Input, Dense, Flatten, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(\n",
    "    per_process_gpu_memory_fraction=0.65, allow_growth=False)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=16,\n",
    "                        inter_op_parallelism_threads=16, allow_soft_placement=True, gpu_options=gpu_options)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    with open('data/ams_word_nomath_index.json') as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "def load_index_vocab():\n",
    "    index_vocab = {}\n",
    "    with open('data/ams_word_nomath_index.json') as json_data:\n",
    "        for word, index in json.load(json_data).items():\n",
    "            index_vocab[index] = word\n",
    "    return index_vocab\n",
    "\n",
    "    \n",
    "def load_glove():\n",
    "    glove = {}\n",
    "    with open('data/glove.model.nomath.txt') as glove_data:\n",
    "        for line in glove_data:\n",
    "            items = line.split()\n",
    "            key = items[0]\n",
    "            glove[key] = np.asarray(items[1:], dtype='float32')\n",
    "    return glove\n",
    "\n",
    "\n",
    "def build_embedding_layer(with_input=False, maxlen=480, vocab_dim=300, mask_zero=True):\n",
    "    print(\"-- loading word embeddings, this may take a couple of minutes...\")\n",
    "    index_dict = load_vocab()\n",
    "    word_vectors = load_glove()\n",
    "    # adding 1 to account for 0th index (for masking)\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    print(\"-- known dictionary items: \", n_symbols)\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    print(\"-- embeddings \")\n",
    "    if not with_input:\n",
    "        embedding_layer = Embedding(\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False, weights=[embedding_weights])\n",
    "        return embedding_layer\n",
    "    else:\n",
    "        # define inputs here\n",
    "        input_1 = Input(shape=(maxlen,), dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "            weights=[embedding_weights],\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False)(input_1)\n",
    "        return (embedding_layer, input_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data is obtained via batch loading from HDF5\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, batch_size=128, mode=\"train\", dim=480,\n",
    "             n_classes=13, x_hf = None, y_hf = None, shuffle=False):\n",
    "        'Initialization'\n",
    "        self.x_hf = x_hf\n",
    "        self.y_hf = y_hf\n",
    "        self.mode = mode\n",
    "        self.total_len = self.y_hf.shape[0]\n",
    "        self.validation_len = int(np.ceil(0.1 * self.total_len))\n",
    "        self.training_len = self.total_len - self.validation_len\n",
    "        if self.mode == \"validation\":\n",
    "            self.data_len = self.validation_len\n",
    "            self.list_IDs = np.arange(self.training_len, self.total_len)\n",
    "        else:\n",
    "            self.data_len = self.training_len\n",
    "            self.list_IDs = np.arange(0,self.training_len+1)\n",
    "            \n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end() \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples,  dim)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim), dtype=int)\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = self.x_hf[ID]\n",
    "            # Store class\n",
    "            y[i] = self.y_hf[ID]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "batch_size = 128\n",
    "# Parameters\n",
    "data_hf = h5py.File(\"data/confusion_free_nomath.hdf5\", 'r')\n",
    "generator_params = {\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 13,\n",
    "    'shuffle': False,\n",
    "    'x_hf': data_hf['x_train'],\n",
    "    'y_hf': data_hf['y_train']\n",
    "}\n",
    "# Generators\n",
    "training_generator = DataGenerator(**generator_params)\n",
    "validation_generator = DataGenerator(mode=\"validation\", **generator_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing word embeddings\n",
    "# Takes 10 minutes to recompute, memoizing here for the v4 49 class variant:\n",
    "# class_weights = compute_class_weight('balanced', np.unique(training_generator.y_hf), training_generator.y_hf)\n",
    "# np.set_printoptions(precision=32, suppress=True)\n",
    "# print(class_weights)\n",
    "\n",
    "class_weights = [\n",
    "  0.7483095715566197, 4.754501247119297,  1.922516556926298,\n",
    "  1.1549558601260375, 2.668737662773207,  1.1207340910837413,\n",
    "  492.8275989186532,  0.3638972379781621, 0.20817024795949343,\n",
    "  13.798952339173741, 29.327446829514418,  1.205880584288014,\n",
    "  3.2184743461345864 ]\n",
    "\n",
    "# 08.2018 (subformula lexemes)\n",
    "# Analyzing the arxiv dataset seems to indicate \n",
    "#   a maxlen of 960 is needed to fit 99.2% of the data\n",
    "#   a maxlen of 480 fits 96.03%, and a maxlen of 300 covers 90.0% of paragraphs\n",
    "maxlen = 480\n",
    "n_classes = 13\n",
    "layer_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loading word embeddings, this may take a couple of minutes...\n",
      "-- known dictionary items:  757203\n",
      "-- embeddings \n"
     ]
    }
   ],
   "source": [
    "embedding_layer = build_embedding_layer(maxlen=maxlen, mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"baseline_nomath_logistic_regression_cat%d\" % (n_classes)\n",
    "\n",
    "# Checkpoints: 1) save best model at epoch end, 2) stop early when metric stops improving\n",
    "checkpoint = ModelCheckpoint(model_file+\"-checkpoint.h5\",\n",
    "                             monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                          min_delta=0.001,\n",
    "                          patience=3,\n",
    "                          restore_best_weights=True,\n",
    "                          verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/deyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/deyan/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "56399/56399 [==============================] - 1877s 33ms/step - loss: 1.2898 - weighted_sparse_categorical_accuracy: 0.7558 - val_loss: 1.4242 - val_weighted_sparse_categorical_accuracy: 0.7452\n",
      "\n",
      "Epoch 00001: val_weighted_sparse_categorical_accuracy improved from -inf to 0.74515, saving model to baseline_nomath_logistic_regression_cat13-checkpoint.h5\n",
      "Epoch 2/500\n",
      "56399/56399 [==============================] - 1860s 33ms/step - loss: 1.3444 - weighted_sparse_categorical_accuracy: 0.7682 - val_loss: 1.5018 - val_weighted_sparse_categorical_accuracy: 0.7473\n",
      "\n",
      "Epoch 00002: val_weighted_sparse_categorical_accuracy improved from 0.74515 to 0.74729, saving model to baseline_nomath_logistic_regression_cat13-checkpoint.h5\n",
      "Epoch 3/500\n",
      "56399/56399 [==============================] - 1877s 33ms/step - loss: 1.3585 - weighted_sparse_categorical_accuracy: 0.7720 - val_loss: 1.5545 - val_weighted_sparse_categorical_accuracy: 0.7369\n",
      "\n",
      "Epoch 00003: val_weighted_sparse_categorical_accuracy did not improve from 0.74729\n",
      "Epoch 4/500\n",
      "56399/56399 [==============================] - 1900s 34ms/step - loss: 1.3605 - weighted_sparse_categorical_accuracy: 0.7750 - val_loss: 1.5239 - val_weighted_sparse_categorical_accuracy: 0.7572\n",
      "\n",
      "Epoch 00004: val_weighted_sparse_categorical_accuracy improved from 0.74729 to 0.75718, saving model to baseline_nomath_logistic_regression_cat13-checkpoint.h5\n",
      "Epoch 5/500\n",
      "56399/56399 [==============================] - 1860s 33ms/step - loss: 1.3676 - weighted_sparse_categorical_accuracy: 0.7765 - val_loss: 1.6236 - val_weighted_sparse_categorical_accuracy: 0.7576\n",
      "\n",
      "Epoch 00005: val_weighted_sparse_categorical_accuracy improved from 0.75718 to 0.75756, saving model to baseline_nomath_logistic_regression_cat13-checkpoint.h5\n",
      "Epoch 6/500\n",
      "56399/56399 [==============================] - 1829s 32ms/step - loss: 1.3681 - weighted_sparse_categorical_accuracy: 0.7783 - val_loss: 1.5825 - val_weighted_sparse_categorical_accuracy: 0.7441\n",
      "\n",
      "Epoch 00006: val_weighted_sparse_categorical_accuracy did not improve from 0.75756\n",
      "Epoch 7/500\n",
      "40724/56399 [====================>.........] - ETA: 7:45 - loss: 1.3605 - weighted_sparse_categorical_accuracy: 0.7795"
     ]
    }
   ],
   "source": [
    "# 1. Baseline: logistic regression with GloVe\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              weighted_metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    workers = 1,\n",
    "    use_multiprocessing=False,\n",
    "    class_weight=class_weights,\n",
    "    epochs=500,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, earlystop])\n",
    "\n",
    "print('Logistic regression model summary:')\n",
    "print(model.summary())\n",
    "\n",
    "print(\"-- saving model to disk : %s \" % model_file)\n",
    "model.save(model_file+'_notebook.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model(model_file+\"_notebook.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-class test measures:\")\n",
    "y_pred = model.predict_classes(data_hf['x_test'], verbose=1, batch_size=batch_size)\n",
    "print(classification_report(data_hf['y_test'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "#     classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n",
    "        annot = True\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        annot = False\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(50,40))\n",
    "    df_cm = pd.DataFrame(cm, index = classes,\n",
    "                  columns = classes)\n",
    "    sn.set(font_scale=1.4)#for label size\n",
    "    sn.heatmap(df_cm, annot=annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "class_names = [\n",
    "    'abstract', 'acknowledgement', 'conclusion', 'definition', 'example',\n",
    "    'introduction', 'keywords', 'proof', 'proposition', 'problem', 'related work', 'remark', 'result']\n",
    "\n",
    "plot_confusion_matrix(data_hf['y_test'], y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(data_hf['y_test'], y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "Converges in 14 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing  NOMATH Linear Regression 13 class\n",
    "```\n",
    "add\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "58738/58738 [==============================] - 1759s 30ms/step - loss: 9.4608 - weighted_sparse_categorical_accuracy: 0.4129 - val_loss: 9.4690 - val_weighted_sparse_categorical_accuracy: 0.4124\n",
      "\n",
      "Epoch 00001: val_weighted_sparse_categorical_accuracy improved from -inf to 0.41243, saving model to baseline_logreg_plain_cat13-checkpoint.h5\n",
      "Epoch 2/500\n",
      "41379/58738 [====================>.........] - ETA: 7:54 - loss: 9.3740 - weighted_sparse_categorical_accuracy: 0.4183"
     ]
    }
   ],
   "source": [
    "# 2. Baseline: logistic regression, plain\n",
    "model_file = \"baseline_nomath_logreg_plain_cat%d\" % (n_classes)\n",
    "\n",
    "# Checkpoints: 1) save best model at epoch end, 2) stop early when metric stops improving\n",
    "checkpoint = ModelCheckpoint(model_file+\"-checkpoint.h5\",\n",
    "                             monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                          min_delta=0.001,\n",
    "                          patience=3,\n",
    "                          restore_best_weights=True,\n",
    "                          verbose=0, mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_classes, input_shape=(480,), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              weighted_metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    workers = 1,\n",
    "    use_multiprocessing=False,\n",
    "    class_weight=class_weights,\n",
    "    epochs=500,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, earlystop])\n",
    "\n",
    "print('Plain Logistic regression model summary:')\n",
    "print(model.summary())\n",
    "\n",
    "print(\"-- saving model to disk : %s \" % model_file)\n",
    "model.save(model_file+'_notebook.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-class test measures:\")\n",
    "y_pred = model.predict_classes(data_hf['x_test'], verbose=1, batch_size=batch_size)\n",
    "print(classification_report(data_hf['y_test'], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
