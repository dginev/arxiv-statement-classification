{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import gc\n",
    "import json\n",
    "import h5py\n",
    "import threading\n",
    "import time\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import Sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import _remove_long_seq\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input, Dense, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# training on 1080 Ti, you may want to adjust these for your own hardware\n",
    "gpu_options = tf.GPUOptions(\n",
    "    per_process_gpu_memory_fraction=0.65, allow_growth=False)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=16,\n",
    "                        inter_op_parallelism_threads=16, allow_soft_placement=True, gpu_options=gpu_options)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index():\n",
    "    f = open('data/ams_word_index.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_vocab():\n",
    "    with open('data/ams_word_index.json') as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "    \n",
    "def load_glove():\n",
    "    glove = {}\n",
    "    with open('data/glove.model.txt') as glove_data:\n",
    "        for line in glove_data:\n",
    "            items = line.split()\n",
    "            key = items[0]\n",
    "            glove[key] = np.asarray(items[1:], dtype='float32')\n",
    "    return glove\n",
    "\n",
    "\n",
    "def build_embedding_layer(with_input=False, maxlen=480, vocab_dim=300, mask_zero=True):\n",
    "    print(\"-- loading word embeddings, this may take a couple of minutes...\")\n",
    "    index_dict = load_vocab()\n",
    "    word_vectors = load_glove()\n",
    "    # adding 1 to account for 0th index (for masking)\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    print(\"-- known dictionary items: \", n_symbols)\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    print(\"-- embeddings \")\n",
    "    if not with_input:\n",
    "        embedding_layer = Embedding(\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False, weights=[embedding_weights])\n",
    "        return embedding_layer\n",
    "    else:\n",
    "        # define inputs here\n",
    "        input_1 = Input(shape=(maxlen,), dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "            weights=[embedding_weights],\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False)(input_1)\n",
    "        return (embedding_layer, input_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lock = threading.Lock()\n",
    "\n",
    "# Input data is obtained via batch loading from HDF5\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, batch_size=128, mode=\"train\", dim=480,\n",
    "             n_classes=49, x_hf = None, y_hf = None, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.x_hf = x_hf\n",
    "        self.y_hf = y_hf\n",
    "        self.mode = mode\n",
    "        self.total_len = self.y_hf.shape[0]\n",
    "        self.validation_len = int(np.ceil(0.1 * self.total_len))\n",
    "        self.training_len = self.total_len - self.validation_len\n",
    "        if self.mode == \"validation\":\n",
    "            self.data_len = self.validation_len\n",
    "            self.list_IDs = np.arange(self.training_len, self.total_len)\n",
    "        else:\n",
    "            self.data_len = self.training_len\n",
    "            self.list_IDs = np.arange(0,self.training_len+1)\n",
    "            \n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end() \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples,  dim)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim), dtype=int)\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = self.x_hf[ID]\n",
    "            # Store class\n",
    "            y[i] = self.y_hf[ID]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "batch_size = 128\n",
    "# Parameters\n",
    "data_hf = h5py.File(\"data/full_ams.hdf5\", 'r')\n",
    "generator_params = {\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 50,\n",
    "    'shuffle': False,\n",
    "    'x_hf': data_hf['x_train'],\n",
    "    'y_hf': data_hf['y_train']\n",
    "}\n",
    "# Generators\n",
    "training_generator = DataGenerator(**generator_params)\n",
    "validation_generator = DataGenerator(mode=\"validation\", **generator_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loading word embeddings, this may take a couple of minutes...\n",
      "-- known dictionary items:  1000296\n",
      "-- embeddings \n"
     ]
    }
   ],
   "source": [
    "# It takes 10 minutes to recompute, memoizing here for the v5 50 class variant:\n",
    "# class_weights = compute_class_weight('balanced', np.unique(training_generator.y_hf), training_generator.y_hf)\n",
    "# np.set_printoptions(precision=32, suppress=True)\n",
    "# print(class_weights)\n",
    "class_weights = [\n",
    "  0.2048110334858486, 1.3013211181655675, 5823.8158620689655, \n",
    "  5277.833125, 7.1379341532479605, 4444.491052631579, \n",
    "  64.83326679462571, 2.35259803033891, 649.5794615384615, \n",
    "  0.7418287154980059, 53.44641139240506, 4.702641309795623, \n",
    "  280.54926910299, 97.06359770114942, 0.48335678225132506, \n",
    "  893.6013756613756, 0.30742495067094794, 9.161413615405479, \n",
    "  1.8098013287612515, 0.7152746908351686, 521.2674691358025, \n",
    "  42222.665, 15353.696363636363, 1362.0214516129033, \n",
    "  12991.58923076923, 11.902090204369275, 21111.3325, \n",
    "  0.30661456290938666, 5117.8987878787875, 134.89669329073482, \n",
    "  0.15985652816716106, 4.142116544857017, 12.710013546056592, \n",
    "  47.3083081232493, 42222.665, 11.243636242593702, \n",
    "  18.71572030141844, 893.6013756613756, 6.951949452539722, \n",
    "  0.09931251242210093, 0.254639873472114, 7.750122063142437, \n",
    "  8.027122623574144, 0.33036140289106664, 0.8799917675317706, \n",
    "  272.40429032258066, 1289.241679389313, 30.55185600578871, \n",
    "  1796.7091489361703, 0.16395304994777288 ]\n",
    "\n",
    "# 08.2018 (subformula lexemes)\n",
    "# Analyzing the arxiv dataset seems to indicate \n",
    "#   a maxlen of 960 is needed to fit 99.2% of the data\n",
    "#   a maxlen of 480 fits 96.03%, and a maxlen of 300 covers 90.0% of paragraphs\n",
    "maxlen = 480\n",
    "# Preparing word embeddings\n",
    "embedding_layer = build_embedding_layer(maxlen=maxlen, mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- setting up model layout...\n",
      "WARNING:tensorflow:From /home/deyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/deyan/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 480, 300)          300088800 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 480, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 480, 256)          440320    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 480, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 480, 128)          164864    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 480, 128)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 64)                49664     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3250      \n",
      "=================================================================\n",
      "Total params: 300,746,898\n",
      "Trainable params: 658,098\n",
      "Non-trainable params: 300,088,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM Model Setup\n",
    "n_classes = 50\n",
    "layer_size = 128  # ~maxlen // 4\n",
    "\n",
    "print(\"-- setting up model layout...\")\n",
    "use_dropout = True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size // 2, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(CuDNNLSTM(layer_size // 2))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              weighted_metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"confusion_bilstm%d_batch%d_cat%d_gpu\" % (\n",
    "    layer_size, batch_size, n_classes)\n",
    "\n",
    "# Checkpoints: 1) save best model at epoch end, 2) stop early when metric stops improving\n",
    "checkpoint = ModelCheckpoint(model_file+\"-checkpoint.h5\",\n",
    "                             monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                          min_delta=0.001,\n",
    "                          patience=3,\n",
    "                          restore_best_weights=True,\n",
    "                          verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- training model...\n",
      "WARNING:tensorflow:From /home/deyan/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "59375/59375 [==============================] - 9712s 164ms/step - loss: 1.0308 - weighted_sparse_categorical_accuracy: 0.6489 - val_loss: 0.9523 - val_weighted_sparse_categorical_accuracy: 0.6711\n",
      "\n",
      "Epoch 00001: val_weighted_sparse_categorical_accuracy improved from -inf to 0.67106, saving model to confusion_bilstm128_batch128_cat50_gpu-checkpoint.h5\n",
      "Epoch 2/50\n",
      "59375/59375 [==============================] - 9723s 164ms/step - loss: 0.9222 - weighted_sparse_categorical_accuracy: 0.6795 - val_loss: 0.9239 - val_weighted_sparse_categorical_accuracy: 0.6789\n",
      "\n",
      "Epoch 00002: val_weighted_sparse_categorical_accuracy improved from 0.67106 to 0.67891, saving model to confusion_bilstm128_batch128_cat50_gpu-checkpoint.h5\n",
      "Epoch 3/50\n",
      "59375/59375 [==============================] - 9691s 163ms/step - loss: 0.9035 - weighted_sparse_categorical_accuracy: 0.6843 - val_loss: 0.9140 - val_weighted_sparse_categorical_accuracy: 0.6816\n",
      "\n",
      "Epoch 00003: val_weighted_sparse_categorical_accuracy improved from 0.67891 to 0.68160, saving model to confusion_bilstm128_batch128_cat50_gpu-checkpoint.h5\n",
      "Epoch 4/50\n",
      "59375/59375 [==============================] - 9689s 163ms/step - loss: 0.8932 - weighted_sparse_categorical_accuracy: 0.6874 - val_loss: 0.9162 - val_weighted_sparse_categorical_accuracy: 0.6816\n",
      "\n",
      "Epoch 00004: val_weighted_sparse_categorical_accuracy did not improve from 0.68160\n",
      "Epoch 5/50\n",
      "30227/59375 [==============>...............] - ETA: 1:15:10 - loss: 0.8858 - weighted_sparse_categorical_accuracy: 0.6896"
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "print('-- training model...')\n",
    "# TODO: How can we make this work with more workers? HDF5 is not thread-safe for reads...\n",
    "# maybe use the unpacked .txt files and map them through the dictionary each time? Unsure... \n",
    "# On a single CPU worker: 2.5 hours per epoch for the full data, with the main BiLSTM model.\n",
    "model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    workers = 1,\n",
    "    use_multiprocessing=False,\n",
    "    class_weight=class_weights,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, earlystop])\n",
    "\n",
    "print(\"-- saving model to disk : %s \" % model_file)\n",
    "model.save(model_file+'_notebook.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "# model = load_model(model_file+\"_notebook.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-class test measures:\")\n",
    "y_pred = model.predict_classes(data_hf['x_test'], verbose=1, batch_size=batch_size)\n",
    "print(classification_report(data_hf['y_test'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=np.array(sorted([\n",
    "    \"abstract\", \"acknowledgement\", \"affirmation\", \"answer\", \"assumption\",\n",
    "    \"bound\", \"case\", \"claim\", \"comment\", \"conclusion\",\n",
    "    \"condition\", \"conjecture\", \"constraint\", \"convention\", \"corollary\",\n",
    "    \"criterion\", \"definition\", \"demonstration\", \"discussion\", \"example\",\n",
    "    \"exercise\", \"expansion\", \"expectation\", \"experiment\", \"explanation\",\n",
    "    \"fact\", \"hint\", \"introduction\", \"issue\", \"keywords\",\n",
    "    \"lemma\", \"method\", \"notation\", \"note\", \"notice\",\n",
    "    \"observation\", \"overview\", \"principle\", \"problem\", \"proof\",\n",
    "    \"proposition\", \"question\", \"related work\", \"remark\", \"result\", \"rule\",\n",
    "    \"solution\", \"step\", \"summary\", \"theorem\"]))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n",
    "        annot = True\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        annot = False\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(50,40))\n",
    "    df_cm = pd.DataFrame(cm, index = classes,\n",
    "                  columns = classes)\n",
    "    sn.set(font_scale=1.4)#for label size\n",
    "    sn.heatmap(df_cm, annot=annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(data_hf['y_test'], y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(data_hf['y_test'], y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class test measures:\n",
    "Using 10,000 paragraphs, training for 50 epochs, then testing on the full 2.1 million testing set yields:\n",
    "```\n",
    "precision    recall  f1-score   support\n",
    "   micro avg       0.52      0.52      0.52   2105855\n",
    "   macro avg       0.13      0.14      0.13   2105855\n",
    "weighted avg       0.49      0.52      0.51   2105855\n",
    "\n",
    "```\n",
    "\n",
    "Using all 8.4 million paragraphs for training (converged in 8 epochs), then testing on the full 2.1 million:\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.94      0.93    206154\n",
    "           1       0.99      1.00      0.99     32445\n",
    "           2       0.00      0.00      0.00         7\n",
    "           3       0.00      0.00      0.00         8\n",
    "           4       0.52      0.36      0.43      5915\n",
    "           5       0.00      0.00      0.00         9\n",
    "           6       0.30      0.04      0.07       651\n",
    "           7       0.44      0.09      0.15     17947\n",
    "           8       0.00      0.00      0.00        65\n",
    "           9       0.73      0.70      0.72     56916\n",
    "          10       0.00      0.00      0.00       789\n",
    "          11       0.58      0.01      0.02      8978\n",
    "          12       0.00      0.00      0.00       150\n",
    "          13       0.10      0.00      0.00       435\n",
    "          14       0.34      0.03      0.05     87353\n",
    "          15       0.00      0.00      0.00        47\n",
    "          16       0.84      0.90      0.87    137342\n",
    "          17       0.67      0.08      0.14      4608\n",
    "          18       0.57      0.30      0.39     23330\n",
    "          19       0.74      0.60      0.66     59029\n",
    "          20       0.00      0.00      0.00        80\n",
    "          21       0.00      0.00      0.00         1\n",
    "          22       0.00      0.00      0.00         2\n",
    "          23       0.00      0.00      0.00        30\n",
    "          24       0.00      0.00      0.00         3\n",
    "          25       1.00      0.00      0.00      3547\n",
    "          26       0.00      0.00      0.00         1\n",
    "          27       0.86      0.90      0.88    137706\n",
    "          28       0.00      0.00      0.00         8\n",
    "          29       0.73      0.87      0.79       313\n",
    "          30       0.45      0.68      0.54    264128\n",
    "          31       0.59      0.50      0.54     10193\n",
    "          32       0.35      0.09      0.14      3322\n",
    "          33       0.00      0.00      0.00       892\n",
    "          35       0.00      0.00      0.00      3755\n",
    "          36       0.62      0.04      0.08      2255\n",
    "          37       0.00      0.00      0.00        47\n",
    "          38       0.71      0.47      0.56      6073\n",
    "          39       0.90      0.94      0.92    425149\n",
    "          40       0.32      0.07      0.12    165813\n",
    "          41       0.64      0.87      0.74      5448\n",
    "          42       0.74      0.72      0.73    127807\n",
    "          43       0.75      0.71      0.73     47985\n",
    "          44       0.60      0.24      0.34       154\n",
    "          45       0.00      0.00      0.00        32\n",
    "          46       0.00      0.00      0.00      1382\n",
    "          47       0.00      0.00      0.00        23\n",
    "          48       0.45      0.65      0.54    257528\n",
    "\n",
    "   micro avg       0.69      0.69      0.69   2105855\n",
    "   macro avg       0.36      0.27      0.27   2105855\n",
    "weighted avg       0.68      0.69      0.66   2105855\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis\n",
    "\n",
    "Strong true signal:\n",
    " - acknowledgement - 1.0\n",
    " - abstract - 0.94\n",
    " - proof - 0.94\n",
    " - definition - 0.9\n",
    " - fion - 0.9\n",
    " - question - 0.87\n",
    " - keywords - 0.87\n",
    " - remark - 0.72 (proof 0.14)\n",
    " - conclusion - 0.7 (abstract 0.14)\n",
    " - result - 0.71\n",
    "\n",
    "Mid-tier true signal:\n",
    " - lemma - 0.68 (theorem 0.24)\n",
    " - theorem - 0.65 (lemma 0.27)\n",
    " - example - 0.6 (0.1 proof + 0.1 remark)\n",
    " - method - 0.5 (result 0.25 + introduction 0.1)\n",
    " - problem - 0.47 (question 0.33)\n",
    "\n",
    "\n",
    "Strong confused signal:\n",
    " - affirmation -> lemma\n",
    " - comment -> 0.72 remark\n",
    " - demonstration -> 0.81 proof\n",
    " - expansion -> 1.0 theorem\n",
    " - expectation -> 0.5 introduction + 0.5 remark\n",
    " - explanation -> 0.65 proof + 0.33 example\n",
    " - hint -> 1.0 proof\n",
    " - issue -> 0.88 question\n",
    "\n",
    "Weak Confused signal:\n",
    " - answer -> remark + proof + example\n",
    " - assumption -> 0.36 self + 0.38 lemma + 0.12 theorem\n",
    " - bound -> 0.44 lemma + 0.56 theorem\n",
    " - case -> lemma + proof\n",
    " - claim -> 0.59 lemma + 0.22 theorem\n",
    " - condition -> 0.51 lemma + 0.21 assumption\n",
    " - conjecture -> 0.6 theorem + 0.26 lemma\n",
    " - constraint -> definition + proof + lemma\n",
    " - convention -> remark + definition\n",
    " - corollary -> 0.39 lemma + 0.49 theorem\n",
    " - criterion -> lemma + theorem\n",
    " - discussion -> 0.38 conclusion +0.3 self + 0.12 result\n",
    " - exercise -> 0.35 problem + 0.12 example + 0.16 result\n",
    " - experiment -> 0.27 example + 0.33 result\n",
    " - fact - 0.53 lemma + 0.33 theorem\n",
    " - note - 0.58 remark+ 0.16 proof\n",
    " - notation -> 0.48 definition + 0.16 proof + 0.12 remark\n",
    " - proposition -> 0.41 lemma + 0.41 theorem\n",
    " - rule -> self, lemma, proof, definition\n",
    " - observation -> 0.39 lemma + 0.24 remark + 0.18 theorem\n",
    " - overview -> 0.26 introduction + 0.25 result\n",
    " - principle -> 0.38 theorem + 0.28 lemma + 0.13 definition\n",
    " - solution -> example, proof, lemma, theorem\n",
    " - step -> proof,lemma,definition\n",
    " - summary -> 0.39 definition\n",
    "\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "I. **Strategy for a \"confusion-free\" classification scheme**\n",
    "  - preserve strong true signal classes\n",
    "    - acknowledgement, abstract, proof, definition, introduction, question, keywords, remark, conclusion, result\n",
    "  - drop any weak confused signal class based on low volume data (< 0.5% of data for biggest class, ~ 10,000 paragraphs)\n",
    "    - notice, expansion, hint, expectation, explanation, affirmation, answer, issue, bound, summary, experiment,\n",
    "    - solution, criterion, principle, comment, exercise, constraint, rule, convention, case,\n",
    "    - step, overview,\n",
    "  - for bigger classes, drop if not combinable in a meaningful way:\n",
    "    - drop: notation, observation, method,\n",
    "  - merge together related mid-tier true signals WHEN related semantically (theorem, lemma)\n",
    "  - add to merged classes stronger, or clearly interpretable signals that would become strong true signals when added.\n",
    "    - BIG \"proposition\" class is clearly motivated, all constituents misclassify to \"theorem+lemma\":\n",
    "      - proposition = lemma + theorem + proposition + assumption + condition + fact + conjecture + claim + corollary\n",
    "    - Enhanced \"remark\" = remark + note\n",
    "    - Enhanced \"proof\" = proof + demonstration\n",
    "    - Enhanced \"question\" = question + problem\n",
    "    - Enhanced \"conclusion\" = conclusion + discussion\n",
    "    \n",
    "  - \"example\" is a bit on the fence... keep it as it has 50,000 entries.\n",
    "\n",
    "  **Final assembly**: 24 of the original classes grouped into 12 strongly separable unions:\n",
    "\n",
    "| Class           | Additional Members | Frequency |\n",
    "|:----------------|:-------------------|----------:|\n",
    "| abstract        | -                  | 1,030,774 |\n",
    "| acknowledgement | -                  |   162,230 |\n",
    "| conclusion      | discussion         |   401,235 |\n",
    "| definition      | -                  |   686,717 |\n",
    "| example         | -                  |   295,152 |\n",
    "| introduction    | -                  |   688,530 |\n",
    "| keywords        | -                  |     1,565 |\n",
    "| proof           | demonstration      | 2,148,793 |\n",
    "| proposition     | assumption, claim, | 4,060,029 |\n",
    "| +               | condition,         |         + |\n",
    "| +               | conjecture,        |         + |\n",
    "| +               | corollary, fact,   |         + |\n",
    "| +               | lemma, theorem     |         + |           \n",
    "| problem         | question           |    57,609 |\n",
    "| remark          | note               |   643,500 |\n",
    "| result          | -                  |   239,931 |\n",
    "\n",
    "  Dropped (25) =\n",
    "  notice, expansion, hint, expectation, explanation, affirmation, answer, issue, bound, summary, experiment,\n",
    "  solution, criterion, principle, comment, exercise, constraint, rule, convention, case, step, overview, notation, observation, method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
