{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import gc\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import Sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import _remove_long_seq\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input, Dense, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gpu_options = tf.GPUOptions(\n",
    "    per_process_gpu_memory_fraction=0.95, allow_growth=False)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=16,\n",
    "                        inter_op_parallelism_threads=16, allow_soft_placement=True, gpu_options=gpu_options)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index():\n",
    "    f = open('data/ams_word_index.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_vocab():\n",
    "    with open('data/ams_word_index.json') as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "    \n",
    "def load_glove():\n",
    "    glove = {}\n",
    "    with open('data/glove.model.txt') as glove_data:\n",
    "        for line in glove_data:\n",
    "            items = line.split()\n",
    "            key = items[0]\n",
    "            glove[key] = np.asarray(items[1:], dtype='float32')\n",
    "    return glove\n",
    "\n",
    "\n",
    "def build_embedding_layer(with_input=False, maxlen=480, vocab_dim=300, mask_zero=True):\n",
    "    print(\"-- loading word embeddings, this may take a little while...\")\n",
    "    index_dict = load_vocab()\n",
    "    word_vectors = load_glove()\n",
    "    # adding 1 to account for 0th index (for masking)\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    print(\"-- known dictionary items: \", n_symbols)\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    print(\"-- embeddings \")\n",
    "    if not with_input:\n",
    "        embedding_layer = Embedding(\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False, weights=[embedding_weights])\n",
    "        return embedding_layer\n",
    "    else:\n",
    "        # define inputs here\n",
    "        input_1 = Input(shape=(maxlen,), dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "            weights=[embedding_weights],\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False)(input_1)\n",
    "        return (embedding_layer, input_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data is obtained via batch loading from HDF5\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, file_name, batch_size=1024, data_split=100):\n",
    "        self.hf = h5py.File(file_name, 'r')\n",
    "        self.total_len = self.hf[\"y_train\"].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.idx = 0\n",
    "        self.len_segment = int(self.total_len / data_split)\n",
    "        self.cur_seg_idx = 0\n",
    "        self.x_cur = self.hf['x_train'][:self.len_segment]\n",
    "        self.y_cur = self.hf['y_train'][:self.len_segment]\n",
    "\n",
    "    def data_size(self):\n",
    "        return self.total_len\n",
    "        \n",
    "    def next_seg(self):\n",
    "        self.cur_seg_idx += self.len_segment\n",
    "        self.x_cur = self.hf['x_train'][self.cur_seg_idx:self.cur_seg_idx+self.len_segment]\n",
    "        self.y_cur = self.hf['y_train'][self.cur_seg_idx:self.cur_seg_idx+self.len_segment]\n",
    "        \n",
    "    def generate(self):\n",
    "        while 1:\n",
    "            idx = self.idx\n",
    "            if idx >= self.len_segment:\n",
    "                self.next_seg()\n",
    "                idx = 0\n",
    "            \n",
    "            if idx + self.batch_size >= self.len_segment:\n",
    "                batch_x = self.x_cur[idx:]\n",
    "                batch_y = self.y_cur[idx:]\n",
    "            else:\n",
    "                batch_x = self.x_cur[idx:(idx + self.batch_size)]\n",
    "                batch_y = self.y_cur[idx:(idx + self.batch_size)]\n",
    "            self.idx = idx + self.batch_size\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "batch_size = 128\n",
    "training_generator = DataGenerator(\"data/full_ams.hdf5\", batch_size=batch_size)\n",
    "train_len = training_generator.data_size()\n",
    "# model_generator = training_generator.generate()\n",
    "\n",
    "# x_len = int(train_len / batch_size)\n",
    "\n",
    "\n",
    "# model.fit_generator(generator=training_generator, \n",
    "#                     epochs=1,\n",
    "#                     steps_per_epoch=x_len, workers=1, \n",
    "#                     use_multiprocessing=False, \n",
    "#                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = training_generator.hf['y_train'][:]\n",
    "new = all[all > 49]\n",
    "len(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-05856ea5c1ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# preparing word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 08.2018 (subformula lexemes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Analyzing the arxiv dataset seems to indicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   a maxlen of 960 is needed to fit 99.2% of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# preparing word embeddings\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "# 08.2018 (subformula lexemes)\n",
    "# Analyzing the arxiv dataset seems to indicate \n",
    "#   a maxlen of 960 is needed to fit 99.2% of the data\n",
    "#   a maxlen of 480 fits 96.03%, and a maxlen of 300 covers 90.0% of paragraphs\n",
    "maxlen = 480\n",
    "embedding_layer = build_embedding_layer(maxlen=maxlen, mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- setting up model layout...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 480, 300)          300088800 \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 480, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 480, 256)          440320    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 480, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 480, 128)          164864    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 480, 128)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)     (None, 64)                49664     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 300,744,168\n",
      "Trainable params: 655,368\n",
      "Non-trainable params: 300,088,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM Model Setup\n",
    "n_classes = 8\n",
    "layer_size = 128  # ~maxlen // 4\n",
    "\n",
    "print(\"-- setting up model layout...\")\n",
    "use_dropout = True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size // 2, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(CuDNNLSTM(layer_size // 2))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              weighted_metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints: 1) save best model at epoch end, 2) stop early when metric stops improving\n",
    "checkpoint = ModelCheckpoint(model_file+\"-checkpoint.h5\",\n",
    "                             monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                          min_delta=0.001,\n",
    "                          patience=3,\n",
    "                          verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- training model...\n",
      "Train on 1957142 samples, validate on 489286 samples\n",
      "Epoch 1/50\n",
      "1957142/1957142 [==============================] - 3298s 2ms/step - loss: 0.4093 - weighted_sparse_categorical_accuracy: 0.8592 - val_loss: 0.2776 - val_weighted_sparse_categorical_accuracy: 0.9093\n",
      "\n",
      "Epoch 00001: val_weighted_sparse_categorical_accuracy improved from -inf to 0.90934, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 2/50\n",
      "1957142/1957142 [==============================] - 3309s 2ms/step - loss: 0.2714 - weighted_sparse_categorical_accuracy: 0.9116 - val_loss: 0.2487 - val_weighted_sparse_categorical_accuracy: 0.9182\n",
      "\n",
      "Epoch 00002: val_weighted_sparse_categorical_accuracy improved from 0.90934 to 0.91820, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 3/50\n",
      "1957142/1957142 [==============================] - 3306s 2ms/step - loss: 0.2500 - weighted_sparse_categorical_accuracy: 0.9185 - val_loss: 0.2381 - val_weighted_sparse_categorical_accuracy: 0.9219\n",
      "\n",
      "Epoch 00003: val_weighted_sparse_categorical_accuracy improved from 0.91820 to 0.92188, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 4/50\n",
      "1957142/1957142 [==============================] - 3312s 2ms/step - loss: 0.2392 - weighted_sparse_categorical_accuracy: 0.9219 - val_loss: 0.2324 - val_weighted_sparse_categorical_accuracy: 0.9243\n",
      "\n",
      "Epoch 00004: val_weighted_sparse_categorical_accuracy improved from 0.92188 to 0.92429, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 5/50\n",
      "1957142/1957142 [==============================] - 3311s 2ms/step - loss: 0.2323 - weighted_sparse_categorical_accuracy: 0.9241 - val_loss: 0.2323 - val_weighted_sparse_categorical_accuracy: 0.9246\n",
      "\n",
      "Epoch 00005: val_weighted_sparse_categorical_accuracy improved from 0.92429 to 0.92460, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 6/50\n",
      "1957142/1957142 [==============================] - 3314s 2ms/step - loss: 0.2276 - weighted_sparse_categorical_accuracy: 0.9256 - val_loss: 0.2293 - val_weighted_sparse_categorical_accuracy: 0.9257\n",
      "\n",
      "Epoch 00006: val_weighted_sparse_categorical_accuracy improved from 0.92460 to 0.92572, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 7/50\n",
      "1957142/1957142 [==============================] - 3312s 2ms/step - loss: 0.2239 - weighted_sparse_categorical_accuracy: 0.9269 - val_loss: 0.2304 - val_weighted_sparse_categorical_accuracy: 0.9253\n",
      "\n",
      "Epoch 00007: val_weighted_sparse_categorical_accuracy did not improve from 0.92572\n",
      "Epoch 8/50\n",
      "1957142/1957142 [==============================] - 3312s 2ms/step - loss: 0.2220 - weighted_sparse_categorical_accuracy: 0.9274 - val_loss: 0.2273 - val_weighted_sparse_categorical_accuracy: 0.9264\n",
      "\n",
      "Epoch 00008: val_weighted_sparse_categorical_accuracy improved from 0.92572 to 0.92644, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 9/50\n",
      "1957142/1957142 [==============================] - 3310s 2ms/step - loss: 0.2201 - weighted_sparse_categorical_accuracy: 0.9279 - val_loss: 0.2266 - val_weighted_sparse_categorical_accuracy: 0.9268\n",
      "\n",
      "Epoch 00009: val_weighted_sparse_categorical_accuracy improved from 0.92644 to 0.92679, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 10/50\n",
      "1957142/1957142 [==============================] - 3308s 2ms/step - loss: 0.2176 - weighted_sparse_categorical_accuracy: 0.9287 - val_loss: 0.2271 - val_weighted_sparse_categorical_accuracy: 0.9272\n",
      "\n",
      "Epoch 00010: val_weighted_sparse_categorical_accuracy improved from 0.92679 to 0.92719, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 11/50\n",
      "1957142/1957142 [==============================] - 3309s 2ms/step - loss: 0.2166 - weighted_sparse_categorical_accuracy: 0.9291 - val_loss: 0.2260 - val_weighted_sparse_categorical_accuracy: 0.9270\n",
      "\n",
      "Epoch 00011: val_weighted_sparse_categorical_accuracy did not improve from 0.92719\n",
      "Epoch 12/50\n",
      "1957142/1957142 [==============================] - 3314s 2ms/step - loss: 0.2168 - weighted_sparse_categorical_accuracy: 0.9289 - val_loss: 0.2246 - val_weighted_sparse_categorical_accuracy: 0.9272\n",
      "\n",
      "Epoch 00012: val_weighted_sparse_categorical_accuracy did not improve from 0.92719\n",
      "Epoch 13/50\n",
      "1957142/1957142 [==============================] - 3310s 2ms/step - loss: 0.2150 - weighted_sparse_categorical_accuracy: 0.9296 - val_loss: 0.2243 - val_weighted_sparse_categorical_accuracy: 0.9275\n",
      "\n",
      "Epoch 00013: val_weighted_sparse_categorical_accuracy improved from 0.92719 to 0.92749, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "-- saving model to disk : bilstm128_batch64_cat8_gpu \n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model_file = \"bilstm%d_batch%d_cat%d_gpu\" % (\n",
    "    layer_size, batch_size, n_classes)\n",
    "\n",
    "# Perform training\n",
    "print('-- training model...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          class_weight=class_weights,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          callbacks=[checkpoint, earlystop],\n",
    "          validation_split=0.2)\n",
    "# serialize model to JSON\n",
    "print(\"-- saving model to disk : %s \" % model_file)\n",
    "model.save(model_file+'_notebook.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class test measures:\n",
      "611608/611608 [==============================] - 398s 650us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9000\n",
      "           1       0.95      0.95      0.95    200000\n",
      "           2       0.92      0.95      0.94    200000\n",
      "           3       0.93      0.94      0.93    141479\n",
      "           4       0.86      0.75      0.80     51574\n",
      "           5       0.84      0.76      0.80      3016\n",
      "           6       0.86      0.82      0.84      6169\n",
      "           7       0.72      0.80      0.76       370\n",
      "\n",
      "   micro avg       0.93      0.93      0.93    611608\n",
      "   macro avg       0.88      0.87      0.88    611608\n",
      "weighted avg       0.93      0.93      0.93    611608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Per-class test measures:\")\n",
    "y_pred = model.predict_classes(x_test, verbose=1, batch_size=batch)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class test measures:\n",
    "```\n",
    "        611608/611608 [==============================] - 398s 650us/step\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "acknowledgement       1.00      1.00      1.00      9000\n",
    "          proof       0.95      0.95      0.95    200000\n",
    "    proposition       0.92      0.95      0.94    200000\n",
    "     definition       0.93      0.94      0.93    141479\n",
    "        example       0.86      0.75      0.80     51574\n",
    "   introduction       0.84      0.76      0.80      3016\n",
    "        problem       0.86      0.82      0.84      6169\n",
    "   related work       0.72      0.80      0.76       370\n",
    "\n",
    "   micro avg       0.93      0.93      0.93    611608\n",
    "   macro avg       0.88      0.87      0.88    611608\n",
    "weighted avg       0.93      0.93      0.93    611608\n",
    "```\n",
    "\n",
    "Trained on 2.44 million paragraphs and evaluated on a test set of 0.6 million paragraphs, for a weighted 0.93 F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
