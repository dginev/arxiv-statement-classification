{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import gc\n",
    "import json\n",
    "import h5py\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import Sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import _remove_long_seq\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input, Dense, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(\n",
    "    per_process_gpu_memory_fraction=0.95, allow_growth=False)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=16,\n",
    "                        inter_op_parallelism_threads=16, allow_soft_placement=True, gpu_options=gpu_options)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index():\n",
    "    f = open('data/ams_word_index.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_vocab():\n",
    "    with open('data/ams_word_index.json') as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "    \n",
    "def load_glove():\n",
    "    glove = {}\n",
    "    with open('data/glove.model.txt') as glove_data:\n",
    "        for line in glove_data:\n",
    "            items = line.split()\n",
    "            key = items[0]\n",
    "            glove[key] = np.asarray(items[1:], dtype='float32')\n",
    "    return glove\n",
    "\n",
    "\n",
    "def build_embedding_layer(with_input=False, maxlen=480, vocab_dim=300, mask_zero=True):\n",
    "    print(\"-- loading word embeddings, this may take a little while...\")\n",
    "    index_dict = load_vocab()\n",
    "    word_vectors = load_glove()\n",
    "    # adding 1 to account for 0th index (for masking)\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    print(\"-- known dictionary items: \", n_symbols)\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    print(\"-- embeddings \")\n",
    "    if not with_input:\n",
    "        embedding_layer = Embedding(\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False, weights=[embedding_weights])\n",
    "        return embedding_layer\n",
    "    else:\n",
    "        # define inputs here\n",
    "        input_1 = Input(shape=(maxlen,), dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "            weights=[embedding_weights],\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False)(input_1)\n",
    "        return (embedding_layer, input_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lock = threading.Lock()\n",
    "\n",
    "# Input data is obtained via batch loading from HDF5\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, batch_size=32, mode=\"train\", dim=480, n_channels=1,\n",
    "             n_classes=49, x_hf = None, y_hf = None, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.x_hf = x_hf\n",
    "        self.y_hf = y_hf\n",
    "        self.total_len = 10_000# self.y_hf.shape[0]\n",
    "        self.validation_len = int(np.floor(0.1 * self.total_len))\n",
    "        self.training_len = self.total_len - self.validation_len\n",
    "        if mode == \"validation\":\n",
    "            self.data_len = self.validation_len\n",
    "            self.list_IDs = np.arange(self.data_len, self.total_len-1)\n",
    "        else:\n",
    "            self.data_len = self.training_len\n",
    "            self.list_IDs = np.arange(0,self.data_len)\n",
    "            \n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()        \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.data_len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.data_len / self.batch_size))\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = self.x_hf[ID]\n",
    "            # Store class\n",
    "            y[i] = self.y_hf[ID]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "batch_size = 128\n",
    "# Parameters\n",
    "data_hf = h5py.File(\"data/full_ams.hdf5\", 'r')\n",
    "generator_params = {\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 49,\n",
    "    'n_channels': 1,\n",
    "    'shuffle': True,\n",
    "    'x_hf': data_hf['x_train'],\n",
    "    'y_hf': data_hf['y_train']\n",
    "}\n",
    "# Generators\n",
    "training_generator = DataGenerator(**generator_params)\n",
    "validation_generator = DataGenerator(mode=\"validate\", **generator_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loading word embeddings, this may take a little while...\n"
     ]
    }
   ],
   "source": [
    "# preparing word embeddings\n",
    "# Takes 10 minutes to recompute, memoizing here for the v4 49 class variant:\n",
    "# class_weights = compute_class_weight('balanced', np.unique(training_generator.hf[\"y_train\"]), training_generator.hf[\"y_train\"])\n",
    "class_weights = [2.0847070797220815e-01,1.3245738287918261e+00,5.9278789584799433e+03,\n",
    " 5.3721403061224491e+03,7.2654786271044491e+00,4.5239076262083781e+03,\n",
    " 6.5991742723960982e+01,2.3946355262772623e+00,6.6118649921507063e+02,\n",
    " 7.5508411281303633e-01,5.4401420821493154e+01,4.7866706519997315e+00,\n",
    " 2.8556227540850227e+02,9.8797982641332396e+01,4.9199366305655895e-01,\n",
    " 9.0956872907893319e+02,3.1291818621247236e-01,9.3251147163503312e+00,\n",
    " 1.8421398392190138e+00,7.2805560645399947e-01,5.3058175862937765e+02,\n",
    " 4.2977122448979593e+04,1.5628044526901669e+04,1.3863587886767609e+03,\n",
    " 1.3223729984301413e+04,1.2114763199148582e+01,2.1488561224489797e+04,\n",
    " 3.1209331800342460e-01,5.2093481756338897e+03,1.3730710047597313e+02,\n",
    " 1.6271293120197022e-01,4.2159233322522649e+00,1.2937122952733171e+01,\n",
    " 4.8153638598296460e+01,4.2977122448979593e+04,1.1444543625319112e+01,\n",
    " 1.9050142929512230e+01,9.0956872907893319e+02,7.0761706510215845e+00,\n",
    " 1.0108708218868796e-01,2.5918991666214608e-01,7.8886054421768703e+00,\n",
    " 3.3626447938485293e-01,8.9561793958612079e-01,2.7727175773535220e+02,\n",
    " 1.3122785480604455e+03,3.1097773117930242e+01,1.8288137212331742e+03,\n",
    " 1.6688265185272180e-01]\n",
    "\n",
    "# 08.2018 (subformula lexemes)\n",
    "# Analyzing the arxiv dataset seems to indicate \n",
    "#   a maxlen of 960 is needed to fit 99.2% of the data\n",
    "#   a maxlen of 480 fits 96.03%, and a maxlen of 300 covers 90.0% of paragraphs\n",
    "maxlen = 480\n",
    "embedding_layer = build_embedding_layer(maxlen=maxlen, mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM Model Setup\n",
    "n_classes = 49\n",
    "layer_size = 128  # ~maxlen // 4\n",
    "\n",
    "print(\"-- setting up model layout...\")\n",
    "use_dropout = True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size // 2, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(CuDNNLSTM(layer_size // 2))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              weighted_metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"confusion_bilstm%d_batch%d_cat%d_gpu\" % (\n",
    "    layer_size, batch_size, n_classes)\n",
    "\n",
    "# Checkpoints: 1) save best model at epoch end, 2) stop early when metric stops improving\n",
    "checkpoint = ModelCheckpoint(model_file+\"-checkpoint.h5\",\n",
    "                             monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                          min_delta=0.001,\n",
    "                          patience=3,\n",
    "                          verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training\n",
    "print('-- training model...')\n",
    "# TODO: How can we make this work with more workers? HDF5 is not thread-safe for reads...\n",
    "# maybe use the unpacked .txt files and map them through the dictionary each time? Unsure... \n",
    "# On a single CPU worker: 2.5 hours per epoch for the full data, with the main BiLSTM model.\n",
    "model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    workers = 1, \n",
    "    use_multiprocessing=True,\n",
    "    class_weight=class_weights,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, earlystop])\n",
    "\n",
    "\n",
    "print(\"-- saving model to disk : %s \" % model_file)\n",
    "model.save(model_file+'_notebook.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-class test measures:\")\n",
    "y_pred = model.predict_classes(data_hf['x_test'], verbose=1, batch_size=batch_size)\n",
    "print(classification_report(data_hf['y_test'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class test measures:\n",
    "Using 10,000 paragraphs, training for 50 epochs yielded `0.8613` val_weighted_sparse_categorical_accuracy.\n",
    "\n",
    "Testing on the full 2.1 million testing set yields:\n",
    "```\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.85      0.82    206154\n",
    "           1       0.96      0.94      0.95     32445\n",
    "           2       0.00      0.00      0.00         7\n",
    "           3       0.00      0.00      0.00         8\n",
    "           4       0.00      0.00      0.00      5915\n",
    "           5       0.00      0.00      0.00         9\n",
    "           6       0.00      0.00      0.00       651\n",
    "           7       0.03      0.01      0.02     17947\n",
    "           8       0.00      0.00      0.00        65\n",
    "           9       0.39      0.35      0.37     56916\n",
    "          10       0.00      0.00      0.00       789\n",
    "          11       0.00      0.00      0.00      8978\n",
    "          12       0.00      0.00      0.00       150\n",
    "          13       0.00      0.00      0.00       435\n",
    "          14       0.00      0.00      0.00     87353\n",
    "          15       0.00      0.00      0.00        47\n",
    "          16       0.58      0.59      0.58    137342\n",
    "          17       0.00      0.00      0.00      4608\n",
    "          18       0.00      0.00      0.00     23330\n",
    "          19       0.17      0.21      0.19     59029\n",
    "          20       0.00      0.00      0.00        80\n",
    "          21       0.00      0.00      0.00         1\n",
    "          22       0.00      0.00      0.00         2\n",
    "          23       0.00      0.00      0.00        30\n",
    "          24       0.00      0.00      0.00         3\n",
    "          25       0.00      0.00      0.00      3547\n",
    "          26       0.00      0.00      0.00         1\n",
    "          27       0.61      0.75      0.67    137706\n",
    "          28       0.00      0.00      0.00         8\n",
    "          29       0.00      0.00      0.00       313\n",
    "          30       0.37      0.43      0.40    264128\n",
    "          31       0.00      0.00      0.00     10193\n",
    "          32       0.00      0.00      0.00      3322\n",
    "          33       0.00      0.00      0.00       892\n",
    "          35       0.00      0.00      0.00      3755\n",
    "          36       0.00      0.00      0.00      2255\n",
    "          37       0.00      0.00      0.00        47\n",
    "          38       0.02      0.00      0.00      6073\n",
    "          39       0.80      0.83      0.81    425149\n",
    "          40       0.19      0.29      0.23    165813\n",
    "          41       0.00      0.00      0.00      5448\n",
    "          42       0.45      0.47      0.46    127807\n",
    "          43       0.56      0.48      0.52     47985\n",
    "          44       0.00      0.00      0.00       154\n",
    "          45       0.00      0.00      0.00        32\n",
    "          46       0.00      0.00      0.00      1382\n",
    "          47       0.00      0.00      0.00        23\n",
    "          48       0.35      0.33      0.34    257528\n",
    "\n",
    "   micro avg       0.52      0.52      0.52   2105855\n",
    "   macro avg       0.13      0.14      0.13   2105855\n",
    "weighted avg       0.49      0.52      0.51   2105855\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=sorted([\n",
    "    \"abstract\", \"acknowledgement\", \"affirmation\", \"answer\", \"assumption\",\n",
    "    \"bound\", \"case\", \"claim\", \"comment\", \"conclusion\",\n",
    "    \"condition\", \"conjecture\", \"constraint\", \"convention\", \"corollary\",\n",
    "    \"criterion\", \"definition\", \"demonstration\", \"discussion\", \"example\",\n",
    "    \"exercise\", \"expansion\", \"expectation\", \"experiment\", \"explanation\",\n",
    "    \"fact\", \"hint\", \"introduction\", \"issue\", \"keywords\",\n",
    "    \"lemma\", \"method\", \"notation\", \"note\", \"notice\",\n",
    "    \"observation\", \"overview\", \"principle\", \"problem\", \"proof\",\n",
    "    \"proposition\", \"question\", \"remark\", \"result\", \"rule\",\n",
    "    \"solution\", \"step\", \"summary\", \"theorem\"])\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(80,60))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i in range(cm.shape[0]):\n",
    "#         for j in range(cm.shape[1]):\n",
    "#             ax.text(j, i, format(cm[i, j], fmt),\n",
    "#                     ha=\"center\", va=\"center\",\n",
    "#                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "#     fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(data_hf['y_test'], y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(data_hf['y_test'], y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
